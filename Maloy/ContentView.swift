import SwiftUI
import AVFoundation
import Combine

struct ContentView: View {
    @StateObject private var audioManager = AudioManager()

    // Helper —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∫–Ω–æ–ø–∫–∏
    private func getButtonText() -> String {
        if audioManager.isAutoMode {
            if audioManager.isProcessing {
                return "‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞..."
            } else if audioManager.isListening {
                return "üõë –ü–†–ï–†–í–ê–¢–¨"
            } else {
                return "üëÇ –°–ª—É—à–∞—é..."
            }
        } else {
            if audioManager.isListening {
                return "üõë –°–¢–û–ü"
            } else if audioManager.isProcessing {
                return "‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞..."
            } else {
                return "üéôÔ∏è –ì–û–í–û–†–ò"
            }
        }
    }

    private func getButtonColor() -> Color {
        if audioManager.isAutoMode {
            if audioManager.isProcessing {
                return .gray
            } else if audioManager.isListening {
                return .red
            } else {
                return .green
            }
        } else {
            if audioManager.isListening {
                return .red
            } else if audioManager.isProcessing {
                return .gray
            } else {
                return .blue
            }
        }
    }

    var body: some View {
        VStack(spacing: 30) {
            Text(audioManager.statusText)
                .font(.title).bold()
                .padding()

            if !audioManager.recognizedText.isEmpty {
                Text("üëÇ \(audioManager.recognizedText)")
                    .foregroundColor(.gray)
                    .padding()
                    .multilineTextAlignment(.center)
            }

            if !audioManager.responseText.isEmpty {
                Text("üí¨ \(audioManager.responseText)")
                    .padding()
                    .multilineTextAlignment(.center)
            }

            Spacer()

            // –ö–Ω–æ–ø–∫–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
            Button(action: {
                if audioManager.isAutoMode {
                    // –í –∞–≤—Ç–æ —Ä–µ–∂–∏–º–µ: –ü–†–ï–†–í–ê–¢–¨ –≤—Å—ë
                    audioManager.interrupt()
                } else {
                    // –í —Ä—É—á–Ω–æ–º —Ä–µ–∂–∏–º–µ: —Å—Ç–∞—Ä—Ç/—Å—Ç–æ–ø
                    if audioManager.isListening {
                        audioManager.stopListening()
                    } else if !audioManager.isProcessing {
                        audioManager.startListening()
                    }
                }
            }) {
                Text(getButtonText())
                    .font(.system(size: 32, weight: .bold))
                    .foregroundColor(.white)
                    .frame(width: 320, height: 120)
                    .background(getButtonColor())
                    .cornerRadius(20)
            }
            .padding(.bottom, 20)

            // –ú–∞–ª–µ–Ω—å–∫–∞—è –∫–Ω–æ–ø–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ä–µ–∂–∏–º–∞
            Button(action: {
                audioManager.isAutoMode.toggle()
                if audioManager.isAutoMode {
                    audioManager.startListeningAuto()
                } else {
                    audioManager.interrupt()
                }
            }) {
                Text(audioManager.isAutoMode ? "ü§ñ –ê–≤—Ç–æ —Ä–µ–∂–∏–º" : "‚úã –†—É—á–Ω–æ–π —Ä–µ–∂–∏–º")
                    .font(.system(size: 16))
                    .foregroundColor(.white)
                    .padding(.horizontal, 20)
                    .padding(.vertical, 10)
                    .background(Color.orange)
                    .cornerRadius(10)
            }
            .padding(.bottom, 30)
        }
        .padding()
        .onAppear { audioManager.sayGreeting() }
    }
}

// MARK: - AUDIO MANAGER

final class AudioManager: NSObject, ObservableObject {
    @Published var recognizedText = ""
    @Published var responseText = ""
    @Published var statusText = "ü§ñ –ú–∞–ª–æ–π"
    @Published var isListening = false
    @Published var isProcessing = false
    @Published var isAutoMode = true  // –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

    // API key is stored in Config.swift (not tracked in git for security)
    private let openAIKey = Config.openAIKey

    private let audioFilename = FileManager.default.temporaryDirectory.appendingPathComponent("input.wav")
    private var audioEngine: AVAudioEngine?
    private var audioFile: AVAudioFile?
    private var player: AVAudioPlayer?
    private var isSpeaking = false

    // VAD (Voice Activity Detection) –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    private var silenceTimer: Timer?
    private var lastSpeechTime = Date()
    private let silenceThreshold: TimeInterval = 1.5  // 1.5 —Å–µ–∫ —Ç–∏—à–∏–Ω—ã ‚Üí —Å—Ç–æ–ø
    private let speechThreshold: Float = -40.0  // –¥–ë, –≤—ã—à–µ –∫–æ—Ç–æ—Ä–æ–≥–æ —Å—á–∏—Ç–∞–µ–º —Ä–µ—á—å—é

    // MARK: –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ
    func sayGreeting() {
        statusText = "üó£Ô∏è –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ..."
        say("–ü—Ä–∏–≤–µ—Ç, —è –ú–∞–ª–æ–π! –ü—Ä–æ—Å—Ç–æ –≥–æ–≤–æ—Ä–∏, —è —Å–ª—É—à–∞—é.") {
            DispatchQueue.main.async {
                if self.isAutoMode {
                    self.startListeningAuto()
                } else {
                    self.statusText = "üí§ –ñ–¥—É –∫–æ–º–∞–Ω–¥—ã"
                }
            }
        }
    }

    // MARK: –°–ª—É—à–∞–Ω–∏–µ (—Ä—É—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –±—É—Ñ–µ—Ä–æ–º)
    func startListening() {
        guard !isSpeaking && !isProcessing else {
            print("‚ö†Ô∏è Cannot start: isSpeaking=\(isSpeaking), isProcessing=\(isProcessing)")
            return
        }

        print("\n========== –ù–ê–ß–ê–õ–û –ó–ê–ü–ò–°–ò ==========")
        recognizedText = ""
        responseText = ""
        isListening = true
        statusText = "üéôÔ∏è –°–ª—É—à–∞—é..."

        // –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –∑–∞–ø–∏—Å—å –ø–µ—Ä–µ–¥ —Å—Ç–∞—Ä—Ç–æ–º –¥–≤–∏–∂–∫–∞
        do {
            try AVAudioSession.sharedInstance().setCategory(.record, mode: .default, options: [.allowBluetoothHFP, .duckOthers])
            try AVAudioSession.sharedInstance().setActive(true)
            print("‚úÖ Audio session configured for recording")
        } catch {
            print("‚ùå Audio session record error:", error)
            stopListening()
            return
        }

        let engine = AVAudioEngine()
        audioEngine = engine
        let input = engine.inputNode

        // –ß–∏—Å—Ç–∏–º –ø—Ä–µ–∂–Ω–∏–π tap
        input.removeTap(onBus: 0)

        // –ü–æ–ª—É—á–∞–µ–º —Ñ–æ—Ä–º–∞—Ç –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å –ª—é–±–æ–π - –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π, –Ω–∞—É—à–Ω–∏–∫–∏, bluetooth)
        let inputFormat = input.outputFormat(forBus: 0)

        print("üé§ Recording format: \(inputFormat.sampleRate)Hz, \(inputFormat.channelCount) ch, \(inputFormat.commonFormat.rawValue)")

        // –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è tap (–±–µ–∑ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏)
        do {
            audioFile = try AVAudioFile(forWriting: audioFilename,
                                        settings: inputFormat.settings,
                                        commonFormat: .pcmFormatFloat32,
                                        interleaved: false)
            print("‚úÖ Audio file created: \(audioFilename.path)")
        } catch {
            print("‚ùå Audio file error:", error)
            stopListening()
            return
        }

        // –£–í–ï–õ–ò–ß–ï–ù–ù–´–ô bufferSize: 4096 ‚Üí 8192 –¥–ª—è –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–π –∑–∞–ø–∏—Å–∏
        // –≠—Ç–æ –¥–∞–µ—Ç –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –∑–∞–ø–∏—Å—å –±—É—Ñ–µ—Ä–∞ –≤ —Ñ–∞–π–ª
        input.installTap(onBus: 0, bufferSize: 8192, format: inputFormat) { [weak self] buffer, time in
            guard let self = self else { return }

            // –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –±—É—Ñ–µ—Ä –≤ —Ñ–∞–π–ª —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –æ—à–∏–±–æ–∫
            do {
                try self.audioFile?.write(from: buffer)
            } catch {
                print("‚ùå File write error at time \(time.sampleTime): \(error)")
            }
        }

        do {
            try engine.start()
            print("‚úÖ Audio engine started with buffer size 8192")
        } catch {
            print("‚ùå Engine start error:", error)
            stopListening()
            return
        }

        print("üéß Recording... Press STOP when done")
    }

    // –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–ª—É—à–∞–Ω–∏—è (–≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –∫–Ω–æ–ø–∫–æ–π)
    func stopListening() {
        guard isListening else { return }

        print("üõë Stopping recording...")
        isListening = false
        statusText = "‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞..."

        // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–≤–∏–∂–æ–∫ –∏ —É–¥–∞–ª—è–µ–º tap
        audioEngine?.stop()
        audioEngine?.inputNode.removeTap(onBus: 0)

        // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–µ—Ä VAD
        silenceTimer?.invalidate()
        silenceTimer = nil

        // –í–∞–∂–Ω–æ: –∑–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        audioFile = nil
        audioEngine = nil

        print("‚úÖ Recording stopped")
        print("========== –ö–û–ù–ï–¶ –ó–ê–ü–ò–°–ò ==========\n")

        transcribeAudio()
    }

    // MARK: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏–µ —Å VAD
    func startListeningAuto() {
        guard !isSpeaking && !isProcessing else {
            print("‚ö†Ô∏è Cannot start auto: isSpeaking=\(isSpeaking), isProcessing=\(isProcessing)")
            return
        }

        print("\n========== AUTO LISTENING (VAD) ==========")
        statusText = "üëÇ –°–ª—É—à–∞—é..."
        lastSpeechTime = Date()  // –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ç–∞–π–º–µ—Ä

        // –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –∑–∞–ø–∏—Å—å
        do {
            try AVAudioSession.sharedInstance().setCategory(.record, mode: .default, options: [.allowBluetoothHFP, .duckOthers])
            try AVAudioSession.sharedInstance().setActive(true)
            print("‚úÖ Audio session configured for VAD recording")
        } catch {
            print("‚ùå Audio session VAD error:", error)
            return
        }

        let engine = AVAudioEngine()
        audioEngine = engine
        let input = engine.inputNode
        input.removeTap(onBus: 0)

        let inputFormat = input.outputFormat(forBus: 0)
        print("üé§ VAD format: \(inputFormat.sampleRate)Hz, \(inputFormat.channelCount) ch")

        // –°–æ–∑–¥–∞—ë–º —Ñ–∞–π–ª –¥–ª—è –∑–∞–ø–∏—Å–∏
        do {
            audioFile = try AVAudioFile(forWriting: audioFilename,
                                        settings: inputFormat.settings,
                                        commonFormat: .pcmFormatFloat32,
                                        interleaved: false)
            print("‚úÖ VAD audio file ready")
        } catch {
            print("‚ùå VAD file error:", error)
            return
        }

        // –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º tap —Å –∞–Ω–∞–ª–∏–∑–æ–º –≥—Ä–æ–º–∫–æ—Å—Ç–∏
        input.installTap(onBus: 0, bufferSize: 8192, format: inputFormat) { [weak self] buffer, time in
            guard let self = self else { return }

            // –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ —Ñ–∞–π–ª
            do {
                try self.audioFile?.write(from: buffer)
            } catch {
                print("‚ùå VAD write error at \(time.sampleTime): \(error)")
            }

            // –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≥—Ä–æ–º–∫–æ—Å—Ç—å (RMS - Root Mean Square)
            guard let channelData = buffer.floatChannelData?[0] else { return }
            let frames = buffer.frameLength
            var sum: Float = 0.0
            for i in 0..<Int(frames) {
                let sample = channelData[i]
                sum += sample * sample
            }
            let rms = sqrt(sum / Float(frames))
            let db = 20 * log10(rms)

            // –ï—Å–ª–∏ –≥—Ä–æ–º–∫–æ—Å—Ç—å –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞ ‚Üí –æ–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ä–µ—á–∏
            if db > self.speechThreshold {
                DispatchQueue.main.async {
                    self.lastSpeechTime = Date()
                    if !self.isListening {
                        self.isListening = true
                        self.statusText = "üéôÔ∏è –ó–∞–ø–∏—Å—ã–≤–∞—é..."
                        print("üó£Ô∏è Speech detected! (level: \(String(format: "%.1f", db))dB)")
                    }
                }
            }
        }

        do {
            try engine.start()
            print("‚úÖ VAD engine started")
        } catch {
            print("‚ùå VAD engine error:", error)
            return
        }

        // –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–∞–π–º–µ—Ä –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–∏—à–∏–Ω—ã (–∫–∞–∂–¥—ã–µ 0.15 —Å–µ–∫)
        silenceTimer = Timer.scheduledTimer(withTimeInterval: 0.15, repeats: true) { [weak self] _ in
            guard let self = self else { return }

            let silenceDuration = Date().timeIntervalSince(self.lastSpeechTime)

            // –ï—Å–ª–∏ –±—ã–ª–æ –Ω–∞—á–∞—Ç–æ –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏–µ –ò –ø—Ä–æ—à–ª–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–∏—à–∏–Ω—ã ‚Üí –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º
            if self.isListening && silenceDuration > self.silenceThreshold {
                print("üîá Silence detected for \(String(format: "%.1f", silenceDuration))s ‚Üí stopping")
                self.stopListeningAuto()
            }
        }
    }

    // –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—è
    func stopListeningAuto() {
        guard isListening else { return }

        print("üõë Auto-stopping recording...")
        isListening = false
        statusText = "‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞..."

        // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–≤–∏–∂–æ–∫
        audioEngine?.stop()
        audioEngine?.inputNode.removeTap(onBus: 0)

        // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–µ—Ä
        silenceTimer?.invalidate()
        silenceTimer = nil

        // –ó–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª
        audioFile = nil
        audioEngine = nil

        print("‚úÖ Auto-recording stopped")
        print("========== END VAD ==========\n")

        transcribeAudio()
    }

    // –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ (–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ–≥–æ)
    func interrupt() {
        print("üõë INTERRUPT - stopping everything")

        // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø–∏—Å—å
        if isListening {
            audioEngine?.stop()
            audioEngine?.inputNode.removeTap(onBus: 0)
            audioFile = nil
            audioEngine = nil
            isListening = false
        }

        // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–µ—Ä
        silenceTimer?.invalidate()
        silenceTimer = nil

        // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ
        player?.stop()
        isSpeaking = false

        isProcessing = false
        statusText = "üõë –ü—Ä–µ—Ä–≤–∞–Ω–æ"

        // –ï—Å–ª–∏ –∞–≤—Ç–æ —Ä–µ–∂–∏–º ‚Üí –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ —Å–µ–∫—É–Ω–¥—É
        if isAutoMode {
            DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) {
                self.startListeningAuto()
            }
        }
    }

    // MARK: Whisper (—Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º)
    private func transcribeAudio() {
        isProcessing = true
        statusText = "üß† –†–∞—Å–ø–æ–∑–Ω–∞—é —Ä–µ—á—å..."
        print("\n========== WHISPER API ==========")

        guard let audioData = try? Data(contentsOf: audioFilename) else {
            print("‚ùå Cannot read audio file")
            DispatchQueue.main.async {
                self.statusText = "‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞"
                self.isProcessing = false
            }
            return
        }

        print("üì¶ Audio file size: \(audioData.count / 1024)KB")

        var req = URLRequest(url: URL(string: "https://api.openai.com/v1/audio/transcriptions")!)
        req.httpMethod = "POST"
        req.addValue("Bearer \(openAIKey)", forHTTPHeaderField: "Authorization")

        let boundary = "Boundary-\(UUID().uuidString)"
        req.setValue("multipart/form-data; boundary=\(boundary)", forHTTPHeaderField: "Content-Type")

        var body = Data()
        body.append("--\(boundary)\r\n".data(using: .utf8)!)
        body.append("Content-Disposition: form-data; name=\"file\"; filename=\"input.wav\"\r\n".data(using: .utf8)!)
        body.append("Content-Type: audio/wav\r\n\r\n".data(using: .utf8)!)
        body.append(audioData)

        // model
        body.append("\r\n--\(boundary)\r\n".data(using: .utf8)!)
        body.append("Content-Disposition: form-data; name=\"model\"\r\n\r\n".data(using: .utf8)!)
        body.append("whisper-1\r\n".data(using: .utf8)!)

        // —è–∑—ã–∫ ‚Äî —Ñ–∏–∫—Å–∏—Ä—É–µ–º —Ä—É—Å—Å–∫–∏–π
        body.append("--\(boundary)\r\n".data(using: .utf8)!)
        body.append("Content-Disposition: form-data; name=\"language\"\r\n\r\n".data(using: .utf8)!)
        body.append("ru\r\n".data(using: .utf8)!)

        // prompt ‚Äî –ø–æ–¥—Å–∫–∞–∑–∫–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (—É–º–µ–Ω—å—à–∞–µ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏)
        body.append("--\(boundary)\r\n".data(using: .utf8)!)
        body.append("Content-Disposition: form-data; name=\"prompt\"\r\n\r\n".data(using: .utf8)!)
        body.append("–†–∞–∑–≥–æ–≤–æ—Ä —Ä–µ–±–µ–Ω–∫–∞ —Å –≥–æ–ª–æ—Å–æ–≤—ã–º –ø–æ–º–æ—â–Ω–∏–∫–æ–º. –í–æ–ø—Ä–æ—Å—ã –ø—Ä–æ —É—á–µ–±—É, –∫–æ—Å–º–æ—Å, –∏–≥—Ä—ã.\r\n".data(using: .utf8)!)

        // temperature ‚Äî —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è (0 = –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ)
        body.append("--\(boundary)\r\n".data(using: .utf8)!)
        body.append("Content-Disposition: form-data; name=\"temperature\"\r\n\r\n".data(using: .utf8)!)
        body.append("0.0\r\n".data(using: .utf8)!)

        body.append("--\(boundary)--\r\n".data(using: .utf8)!)
        req.httpBody = body

        print("üì§ Sending to Whisper API...")
        let startTime = Date()

        URLSession.shared.dataTask(with: req) { data, response, error in
            let elapsed = Date().timeIntervalSince(startTime)
            print("‚è±Ô∏è Whisper response time: \(String(format: "%.1f", elapsed))s")

            if let error = error {
                print("‚ùå Whisper error: \(error.localizedDescription)")
                DispatchQueue.main.async {
                    self.statusText = "‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏"
                    self.isProcessing = false
                }
                return
            }

            guard let data = data else {
                print("‚ùå No data received")
                DispatchQueue.main.async {
                    self.statusText = "‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
                    self.isProcessing = false
                }
                return
            }

            guard let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any] else {
                print("‚ùå Cannot parse JSON")
                if let responseString = String(data: data, encoding: .utf8) {
                    print("Raw response: \(responseString)")
                }
                DispatchQueue.main.async {
                    self.statusText = "‚ùå –û—à–∏–±–∫–∞ API"
                    self.isProcessing = false
                }
                return
            }

            guard let text = json["text"] as? String else {
                print("‚ùå No 'text' field in response")
                print("JSON: \(json)")
                DispatchQueue.main.async {
                    self.statusText = "‚ùå –ù–µ—Ç —Ç–µ–∫—Å—Ç–∞"
                    self.isProcessing = false
                }
                return
            }

            let trimmedText = text.trimmingCharacters(in: .whitespacesAndNewlines)
            print("‚úÖ Recognized: \"\(trimmedText)\"")
            print("========== END WHISPER ==========\n")

            if trimmedText.isEmpty {
                print("‚ö†Ô∏è Empty recognition - nothing heard")
                DispatchQueue.main.async {
                    self.statusText = "ü§∑ –ù–∏—á–µ–≥–æ –Ω–µ —É—Å–ª—ã—à–∞–ª"
                    self.recognizedText = "(–ø—É—Å—Ç–æ)"
                    self.isProcessing = false
                }
                return
            }

            DispatchQueue.main.async {
                self.recognizedText = trimmedText
                self.askGPT(trimmedText)
            }
        }.resume()
    }

    // MARK: GPT (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
    private func askGPT(_ text: String) {
        guard !text.isEmpty else {
            print("‚ö†Ô∏è Empty text for GPT")
            DispatchQueue.main.async {
                self.isProcessing = false
                self.statusText = "üí§ –ñ–¥—É –∫–æ–º–∞–Ω–¥—ã"
            }
            return
        }

        // –°—Ä–∞–∑—É –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å
        DispatchQueue.main.async {
            self.statusText = "ü§î –î—É–º–∞—é..."
        }

        print("\n========== GPT API ==========")
        print("üìù User input: \"\(text)\"")

        let url = URL(string: "https://api.openai.com/v1/chat/completions")!
        let systemPrompt = """
        –¢—ã –≥–æ–ª–æ—Å–æ–≤–æ–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –∏–º–µ–Ω–∏ –ú–∞–ª–æ–π. –¢—ã –∫—Ä—É—Ç–æ–π –∏ –≤–µ—Å—ë–ª—ã–π.
        –†–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞–π —Å —Ä–µ–±—ë–Ω–∫–æ–º 5 –∫–ª–∞—Å—Å–∞ –ø—Ä–æ—Å—Ç–æ, –¥—Ä—É–∂–µ–ª—é–±–Ω–æ –∏ –∫–æ—Ä–æ—Ç–∫–æ.
        –†–µ–±—ë–Ω–∫–∞ –∑–æ–≤—É—Ç –§—ë–¥–æ—Ä. –û–Ω –Ω–µ–∑—Ä—è—á–∏–π. –ü–æ—ç—Ç–æ–º—É –æ–ø–∏—Å—ã–≤–∞–π –ø—Ä–µ–¥–º–µ—Ç—ã —Ç–∞–∫—Ç–∏–ª—å–Ω–æ ‚Äî —Ñ–æ—Ä–º—É, —Ä–∞–∑–º–µ—Ä, –æ—â—É—â–µ–Ω–∏—è.
        –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –¥–ª–∏–Ω–Ω—ã–π ‚Äî –æ—Ç–≤–µ—á–∞–π —á—É—Ç—å –ø–æ–¥—Ä–æ–±–Ω–µ–µ.
        –ï—Å–ª–∏ –ø—Ä–æ—Å—è—Ç –ø–æ—á–∏—Ç–∞—Ç—å –∫–Ω–∏–≥—É ‚Äî —á–∏—Ç–∞–π —á–∞—Å—Ç—è–º–∏ (–ø–æ 3‚Äì4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).
        –ì–æ–≤–æ—Ä–∏ –ø–æ-—Ä—É—Å—Å–∫–∏. –ò–Ω–æ–≥–¥–∞ –≤—Å—Ç–∞–≤–ª—è–π –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è —Å –∫–æ—Ä–æ—Ç–∫–∏–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º.
        –û—Ç–≤–µ—á–∞–π –Ω–µ –¥–æ–ª—å—à–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.
        """

        let body: [String: Any] = [
            "model": "gpt-3.5-turbo",  // –ü–µ—Ä–µ–∫–ª—é—á–∏–ª–∏—Å—å —Å gpt-4o-mini –Ω–∞ gpt-3.5-turbo (–±—ã—Å—Ç—Ä–µ–µ –≤ 3-4 —Ä–∞–∑–∞!)
            "messages": [
                ["role": "system", "content": systemPrompt],
                ["role": "user", "content": text]
            ],
            "max_tokens": 80
        ]

        var req = URLRequest(url: url)
        req.httpMethod = "POST"
        req.addValue("Bearer \(openAIKey)", forHTTPHeaderField: "Authorization")
        req.addValue("application/json", forHTTPHeaderField: "Content-Type")
        req.httpBody = try? JSONSerialization.data(withJSONObject: body)

        print("üì§ Sending to GPT...")
        let startTime = Date()

        URLSession.shared.dataTask(with: req) { data, response, error in
            let elapsed = Date().timeIntervalSince(startTime)
            print("‚è±Ô∏è GPT response time: \(String(format: "%.1f", elapsed))s")

            if let error = error {
                print("‚ùå GPT error: \(error.localizedDescription)")
                DispatchQueue.main.async {
                    self.statusText = "‚ùå –û—à–∏–±–∫–∞ GPT"
                    self.isProcessing = false
                }
                return
            }

            guard let data = data,
                  let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any] else {
                print("‚ùå Cannot parse GPT response")
                DispatchQueue.main.async {
                    self.statusText = "‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞"
                    self.isProcessing = false
                }
                return
            }

            guard let choices = json["choices"] as? [[String: Any]],
                  let msg = choices.first?["message"] as? [String: Any],
                  let reply = msg["content"] as? String else {
                print("‚ùå No content in GPT response")
                print("JSON: \(json)")
                DispatchQueue.main.async {
                    self.statusText = "‚ùå –ù–µ—Ç –æ—Ç–≤–µ—Ç–∞"
                    self.isProcessing = false
                }
                return
            }

            print("‚úÖ GPT reply: \"\(reply)\"")
            print("========== END GPT ==========\n")

            // –ú–û–ú–ï–ù–¢–ê–õ–¨–ù–û –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
            DispatchQueue.main.async {
                self.responseText = reply
                self.statusText = "üó£Ô∏è –ì–æ—Ç–æ–≤–ª—é –æ–∑–≤—É—á–∫—É..."
            }

            // TTS –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ (–Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç UI)
            self.say(reply) {
                DispatchQueue.main.async {
                    self.isProcessing = false
                    // –ï—Å–ª–∏ –∞–≤—Ç–æ —Ä–µ–∂–∏–º ‚Üí –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å–ª—É—à–∞—Ç—å
                    if self.isAutoMode {
                        self.startListeningAuto()
                    } else {
                        self.statusText = "üí§ –ñ–¥—É –∫–æ–º–∞–Ω–¥—ã"
                    }
                }
            }
        }.resume()
    }

    // MARK: TTS (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
    func say(_ text: String, completion: (() -> Void)? = nil) {
        print("\n========== TTS API ==========")
        print("üí¨ Text to speak: \"\(text)\"")

        // –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å —Ç–æ–ª—å–∫–æ –≤ main thread
        DispatchQueue.main.async {
            self.statusText = "üó£Ô∏è –ì–æ–≤–æ—Ä—é..."
        }

        guard let url = URL(string: "https://api.openai.com/v1/audio/speech") else {
            print("‚ùå Invalid TTS URL")
            completion?()
            return
        }

        let json: [String: Any] = [
            "model": "gpt-4o-mini-tts",
            "voice": "alloy",
            "input": text,
            "speed": 1.15  // –£–≤–µ–ª–∏—á–∏–ª–∏ —Å–∫–æ—Ä–æ—Å—Ç—å —Å 1.0 –¥–æ 1.15 –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ä–µ—á–∏
        ]

        var req = URLRequest(url: url)
        req.httpMethod = "POST"
        req.addValue("Bearer \(openAIKey)", forHTTPHeaderField: "Authorization")
        req.addValue("application/json", forHTTPHeaderField: "Content-Type")
        req.httpBody = try? JSONSerialization.data(withJSONObject: json)

        print("üì§ Requesting TTS...")
        let startTime = Date()

        URLSession.shared.dataTask(with: req) { data, response, error in
            let elapsed = Date().timeIntervalSince(startTime)
            print("‚è±Ô∏è TTS response time: \(String(format: "%.1f", elapsed))s")

            if let error = error {
                print("‚ùå TTS error:", error.localizedDescription)
                DispatchQueue.main.async {
                    self.statusText = "‚ùå –û—à–∏–±–∫–∞ TTS"
                }
                completion?()
                return
            }

            guard let data = data, !data.isEmpty else {
                print("‚ùå Empty TTS response")
                DispatchQueue.main.async {
                    self.statusText = "‚ùå –ù–µ—Ç –∞—É–¥–∏–æ"
                }
                completion?()
                return
            }

            print("‚úÖ TTS received (\(data.count / 1024)KB)")

            let tmp = FileManager.default.temporaryDirectory.appendingPathComponent("speech.mp3")
            do {
                try data.write(to: tmp, options: .atomic)
                print("‚úÖ TTS file saved: \(tmp.path)")
                DispatchQueue.main.async {
                    self.playAudio(from: tmp, completion: completion)
                }
            } catch {
                print("‚ùå TTS write error:", error)
                completion?()
            }
        }.resume()
    }

    private func playAudio(from url: URL, completion: (() -> Void)? = nil) {
        print("üîä Starting playback...")

        do {
            // –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –ø–µ—Ä–µ–¥ TTS
            try AVAudioSession.sharedInstance().setCategory(.playback, mode: .default)
            try AVAudioSession.sharedInstance().setActive(true)
            print("‚úÖ Audio session configured for playback")

            isSpeaking = true
            let p = try AVAudioPlayer(contentsOf: url)
            player = p

            p.prepareToPlay()
            let success = p.play()

            if !success {
                print("‚ùå Failed to start audio playback")
                self.isSpeaking = false
                DispatchQueue.main.async {
                    self.statusText = "‚ùå –û—à–∏–±–∫–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è"
                }
                completion?()
                return
            }

            print("üîä Playing audio (duration: \(String(format: "%.1f", p.duration))s)")

            // –ñ–¥–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è + –Ω–µ–±–æ–ª—å—à–æ–π –±—É—Ñ–µ—Ä
            DispatchQueue.main.asyncAfter(deadline: .now() + p.duration + 0.3) {
                print("‚úÖ Playback finished")
                print("========== END TTS ==========\n")
                self.isSpeaking = false
                completion?()
            }
        } catch {
            print("‚ùå TTS play error:", error)
            self.isSpeaking = false
            DispatchQueue.main.async {
                self.statusText = "‚ùå –û—à–∏–±–∫–∞ –ø–ª–µ–µ—Ä–∞"
            }
            completion?()
        }
    }
}
