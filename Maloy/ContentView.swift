import SwiftUI
import AVFoundation
import Combine

struct ContentView: View {
    @StateObject private var audioManager = AudioManager()

    // Helper —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∫–Ω–æ–ø–∫–∏
    private func getButtonText() -> String {
        if audioManager.isAutoMode {
            if audioManager.isProcessing {
                return "‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞..."
            } else if audioManager.isListening {
                return "üõë –ü–†–ï–†–í–ê–¢–¨"
            } else {
                return "üëÇ –°–ª—É—à–∞—é..."
            }
        } else {
            if audioManager.isListening {
                return "üõë –°–¢–û–ü"
            } else if audioManager.isProcessing {
                return "‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞..."
            } else {
                return "üéôÔ∏è –ì–û–í–û–†–ò"
            }
        }
    }

    private func getButtonColor() -> Color {
        if audioManager.isAutoMode {
            if audioManager.isProcessing {
                return .gray
            } else if audioManager.isListening {
                return .red
            } else {
                return .green
            }
        } else {
            if audioManager.isListening {
                return .red
            } else if audioManager.isProcessing {
                return .gray
            } else {
                return .blue
            }
        }
    }

    var body: some View {
        VStack(spacing: 30) {
            Text(audioManager.statusText)
                .font(.title).bold()
                .padding()

            if !audioManager.recognizedText.isEmpty {
                Text("üëÇ \(audioManager.recognizedText)")
                    .foregroundColor(.gray)
                    .padding()
                    .multilineTextAlignment(.center)
            }

            if !audioManager.responseText.isEmpty {
                Text("üí¨ \(audioManager.responseText)")
                    .padding()
                    .multilineTextAlignment(.center)
            }

            Spacer()

            // –ö–Ω–æ–ø–∫–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
            Button(action: {
                if audioManager.isAutoMode {
                    // –í –∞–≤—Ç–æ —Ä–µ–∂–∏–º–µ: –ü–†–ï–†–í–ê–¢–¨ –≤—Å—ë
                    audioManager.interrupt()
                } else {
                    // –í —Ä—É—á–Ω–æ–º —Ä–µ–∂–∏–º–µ: —Å—Ç–∞—Ä—Ç/—Å—Ç–æ–ø
                    if audioManager.isListening {
                        audioManager.stopListening()
                    } else if !audioManager.isProcessing {
                        audioManager.startListening()
                    }
                }
            }) {
                Text(getButtonText())
                    .font(.system(size: 32, weight: .bold))
                    .foregroundColor(.white)
                    .frame(width: 320, height: 120)
                    .background(getButtonColor())
                    .cornerRadius(20)
            }
            .padding(.bottom, 20)

            // –ú–∞–ª–µ–Ω—å–∫–∏–µ –∫–Ω–æ–ø–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
            HStack(spacing: 15) {
                // –ö–Ω–æ–ø–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ä–µ–∂–∏–º–∞
                Button(action: {
                    audioManager.isAutoMode.toggle()
                    if audioManager.isAutoMode {
                        audioManager.startListeningAuto()
                    } else {
                        audioManager.interrupt()
                    }
                }) {
                    Text(audioManager.isAutoMode ? "ü§ñ –ê–≤—Ç–æ" : "‚úã –†—É—á–Ω–æ–π")
                        .font(.system(size: 16))
                        .foregroundColor(.white)
                        .padding(.horizontal, 20)
                        .padding(.vertical, 10)
                        .background(Color.orange)
                        .cornerRadius(10)
                }

                // –ö–Ω–æ–ø–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏
                Button(action: {
                    audioManager.clearHistory()
                }) {
                    Text("üóëÔ∏è –ù–æ–≤—ã–π —Ä–∞–∑–≥–æ–≤–æ—Ä")
                        .font(.system(size: 16))
                        .foregroundColor(.white)
                        .padding(.horizontal, 20)
                        .padding(.vertical, 10)
                        .background(Color.purple)
                        .cornerRadius(10)
                }
            }
            .padding(.bottom, 30)
        }
        .padding()
        .onAppear { audioManager.sayGreeting() }
    }
}

// MARK: - AUDIO MANAGER

final class AudioManager: NSObject, ObservableObject {
    @Published var recognizedText = ""
    @Published var responseText = ""
    @Published var statusText = "ü§ñ –ú–∞–ª–æ–π"
    @Published var isListening = false
    @Published var isProcessing = false
    @Published var isAutoMode = true  // –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

    // API key is stored in Config.swift (not tracked in git for security)
    private let openAIKey = Config.openAIKey

    private let audioFilename = FileManager.default.temporaryDirectory.appendingPathComponent("input.wav")
    private var audioEngine: AVAudioEngine?
    private var audioFile: AVAudioFile?
    private var player: AVAudioPlayer?
    private var isSpeaking = false

    // VAD (Voice Activity Detection) –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    private var silenceTimer: Timer?
    private var lastSpeechTime = Date()
    private let silenceThreshold: TimeInterval = 1.5  // 1.5 —Å–µ–∫ —Ç–∏—à–∏–Ω—ã ‚Üí —Å—Ç–æ–ø
    private let speechThreshold: Float = -40.0  // –¥–ë, –≤—ã—à–µ –∫–æ—Ç–æ—Ä–æ–≥–æ —Å—á–∏—Ç–∞–µ–º —Ä–µ—á—å—é

    // –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ GPT
    private var conversationHistory: [[String: String]] = []
    private let maxHistoryPairs = 4  // –•—Ä–∞–Ω–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 4 –ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç (8 —Å–æ–æ–±—â–µ–Ω–∏–π)

    // MARK: –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ
    func sayGreeting() {
        // –ó–∞–ø—Ä–µ—â–∞–µ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫—É —ç–∫—Ä–∞–Ω–∞ –ø–æ–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ
        UIApplication.shared.isIdleTimerDisabled = true
        print("‚úÖ Screen lock disabled - device will stay awake")

        statusText = "üó£Ô∏è –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ..."
        say("–ü—Ä–∏–≤–µ—Ç, —è –ú–∞–ª–æ–π! –ü—Ä–æ—Å—Ç–æ –≥–æ–≤–æ—Ä–∏, —è —Å–ª—É—à–∞—é.") {
            DispatchQueue.main.async {
                if self.isAutoMode {
                    self.startListeningAuto()
                } else {
                    self.statusText = "üí§ –ñ–¥—É –∫–æ–º–∞–Ω–¥—ã"
                }
            }
        }
    }

    // MARK: –°–ª—É—à–∞–Ω–∏–µ (—Ä—É—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –±—É—Ñ–µ—Ä–æ–º)
    func startListening() {
        guard !isSpeaking && !isProcessing else {
            print("‚ö†Ô∏è Cannot start: isSpeaking=\(isSpeaking), isProcessing=\(isProcessing)")
            return
        }

        print("\n========== –ù–ê–ß–ê–õ–û –ó–ê–ü–ò–°–ò ==========")
        recognizedText = ""
        responseText = ""
        isListening = true
        statusText = "üéôÔ∏è –°–ª—É—à–∞—é..."

        // –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –∑–∞–ø–∏—Å—å –ø–µ—Ä–µ–¥ —Å—Ç–∞—Ä—Ç–æ–º –¥–≤–∏–∂–∫–∞
        do {
            try AVAudioSession.sharedInstance().setCategory(.record, mode: .default, options: [.allowBluetoothHFP, .duckOthers])
            try AVAudioSession.sharedInstance().setActive(true)
            print("‚úÖ Audio session configured for recording")
        } catch {
            print("‚ùå Audio session record error:", error)
            stopListening()
            return
        }

        let engine = AVAudioEngine()
        audioEngine = engine
        let input = engine.inputNode

        // –ß–∏—Å—Ç–∏–º –ø—Ä–µ–∂–Ω–∏–π tap
        input.removeTap(onBus: 0)

        // –ü–æ–ª—É—á–∞–µ–º —Ñ–æ—Ä–º–∞—Ç –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å –ª—é–±–æ–π - –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π, –Ω–∞—É—à–Ω–∏–∫–∏, bluetooth)
        let inputFormat = input.outputFormat(forBus: 0)

        print("üé§ Recording format: \(inputFormat.sampleRate)Hz, \(inputFormat.channelCount) ch, \(inputFormat.commonFormat.rawValue)")

        // –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è tap (–±–µ–∑ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏)
        do {
            audioFile = try AVAudioFile(forWriting: audioFilename,
                                        settings: inputFormat.settings,
                                        commonFormat: .pcmFormatFloat32,
                                        interleaved: false)
            print("‚úÖ Audio file created: \(audioFilename.path)")
        } catch {
            print("‚ùå Audio file error:", error)
            stopListening()
            return
        }

        // –£–í–ï–õ–ò–ß–ï–ù–ù–´–ô bufferSize: 4096 ‚Üí 8192 –¥–ª—è –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–π –∑–∞–ø–∏—Å–∏
        // –≠—Ç–æ –¥–∞–µ—Ç –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –∑–∞–ø–∏—Å—å –±—É—Ñ–µ—Ä–∞ –≤ —Ñ–∞–π–ª
        input.installTap(onBus: 0, bufferSize: 8192, format: inputFormat) { [weak self] buffer, time in
            guard let self = self else { return }

            // –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –±—É—Ñ–µ—Ä –≤ —Ñ–∞–π–ª —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –æ—à–∏–±–æ–∫
            do {
                try self.audioFile?.write(from: buffer)
            } catch {
                print("‚ùå File write error at time \(time.sampleTime): \(error)")
            }
        }

        do {
            try engine.start()
            print("‚úÖ Audio engine started with buffer size 8192")
        } catch {
            print("‚ùå Engine start error:", error)
            stopListening()
            return
        }

        print("üéß Recording... Press STOP when done")
    }

    // –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–ª—É—à–∞–Ω–∏—è (–≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –∫–Ω–æ–ø–∫–æ–π)
    func stopListening() {
        guard isListening else { return }

        print("üõë Stopping recording...")
        isListening = false
        statusText = "‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞..."

        // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–≤–∏–∂–æ–∫ –∏ —É–¥–∞–ª—è–µ–º tap
        audioEngine?.stop()
        audioEngine?.inputNode.removeTap(onBus: 0)

        // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–µ—Ä VAD
        silenceTimer?.invalidate()
        silenceTimer = nil

        // –í–∞–∂–Ω–æ: –∑–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        audioFile = nil
        audioEngine = nil

        print("‚úÖ Recording stopped")
        print("========== –ö–û–ù–ï–¶ –ó–ê–ü–ò–°–ò ==========\n")

        transcribeAudio()
    }

    // MARK: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏–µ —Å VAD
    func startListeningAuto() {
        guard !isSpeaking && !isProcessing else {
            print("‚ö†Ô∏è Cannot start auto: isSpeaking=\(isSpeaking), isProcessing=\(isProcessing)")
            return
        }

        print("\n========== AUTO LISTENING (VAD) ==========")

        // ‚úÖ –û–ß–ò–©–ê–ï–ú –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ –Ω–æ–≤–æ–π –∑–∞–ø–∏—Å—å—é (–∫–∞–∫ –≤ —Ä—É—á–Ω–æ–º —Ä–µ–∂–∏–º–µ)
        recognizedText = ""
        responseText = ""

        statusText = "üëÇ –°–ª—É—à–∞—é..."
        lastSpeechTime = Date()  // –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ç–∞–π–º–µ—Ä

        // –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –∑–∞–ø–∏—Å—å
        do {
            try AVAudioSession.sharedInstance().setCategory(.record, mode: .default, options: [.allowBluetoothHFP, .duckOthers])
            try AVAudioSession.sharedInstance().setActive(true)
            print("‚úÖ Audio session configured for VAD recording")
        } catch {
            print("‚ùå Audio session VAD error:", error)
            return
        }

        let engine = AVAudioEngine()
        audioEngine = engine
        let input = engine.inputNode
        input.removeTap(onBus: 0)

        let inputFormat = input.outputFormat(forBus: 0)
        print("üé§ VAD format: \(inputFormat.sampleRate)Hz, \(inputFormat.channelCount) ch")

        // –°–æ–∑–¥–∞—ë–º —Ñ–∞–π–ª –¥–ª—è –∑–∞–ø–∏—Å–∏
        do {
            audioFile = try AVAudioFile(forWriting: audioFilename,
                                        settings: inputFormat.settings,
                                        commonFormat: .pcmFormatFloat32,
                                        interleaved: false)
            print("‚úÖ VAD audio file ready")
        } catch {
            print("‚ùå VAD file error:", error)
            return
        }

        // –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º tap —Å –∞–Ω–∞–ª–∏–∑–æ–º –≥—Ä–æ–º–∫–æ—Å—Ç–∏
        input.installTap(onBus: 0, bufferSize: 8192, format: inputFormat) { [weak self] buffer, time in
            guard let self = self else { return }

            // –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ —Ñ–∞–π–ª
            do {
                try self.audioFile?.write(from: buffer)
            } catch {
                print("‚ùå VAD write error at \(time.sampleTime): \(error)")
            }

            // –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≥—Ä–æ–º–∫–æ—Å—Ç—å (RMS - Root Mean Square)
            guard let channelData = buffer.floatChannelData?[0] else { return }
            let frames = buffer.frameLength
            var sum: Float = 0.0
            for i in 0..<Int(frames) {
                let sample = channelData[i]
                sum += sample * sample
            }
            let rms = sqrt(sum / Float(frames))
            let db = 20 * log10(rms)

            // –ï—Å–ª–∏ –≥—Ä–æ–º–∫–æ—Å—Ç—å –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞ ‚Üí –æ–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ä–µ—á–∏
            if db > self.speechThreshold {
                DispatchQueue.main.async {
                    self.lastSpeechTime = Date()
                    if !self.isListening {
                        self.isListening = true
                        self.statusText = "üéôÔ∏è –ó–∞–ø–∏—Å—ã–≤–∞—é..."
                        print("üó£Ô∏è Speech detected! (level: \(String(format: "%.1f", db))dB)")
                    }
                }
            }
        }

        do {
            try engine.start()
            print("‚úÖ VAD engine started")
        } catch {
            print("‚ùå VAD engine error:", error)
            return
        }

        // –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–∞–π–º–µ—Ä –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–∏—à–∏–Ω—ã (–∫–∞–∂–¥—ã–µ 0.15 —Å–µ–∫)
        silenceTimer = Timer.scheduledTimer(withTimeInterval: 0.15, repeats: true) { [weak self] _ in
            guard let self = self else { return }

            let silenceDuration = Date().timeIntervalSince(self.lastSpeechTime)

            // –ï—Å–ª–∏ –±—ã–ª–æ –Ω–∞—á–∞—Ç–æ –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏–µ –ò –ø—Ä–æ—à–ª–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–∏—à–∏–Ω—ã ‚Üí –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º
            if self.isListening && silenceDuration > self.silenceThreshold {
                print("üîá Silence detected for \(String(format: "%.1f", silenceDuration))s ‚Üí stopping")
                self.stopListeningAuto()
            }
        }
    }

    // –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—è
    func stopListeningAuto() {
        guard isListening else { return }

        print("üõë Auto-stopping recording...")
        isListening = false

        // –ú–ì–ù–û–í–ï–ù–ù–ê–Ø –†–ï–ê–ö–¶–ò–Ø - –≥–æ–≤–æ—Ä–∏–º —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–ø–∏—Å–∏!
        let quickReactions = ["–ê–≥–∞", "–ü–æ–Ω—è–ª", "–¢–∞–∫-—Ç–∞–∫", "–Ø—Å–Ω–æ", "–û–∫–µ–π", "–•–º", "–°–µ–∫—É–Ω–¥—É"]
        let reaction = quickReactions.randomElement() ?? "–ê–≥–∞"

        print("üí¨ Quick reaction (before transcription): \"\(reaction)\"")

        // –ì–æ–≤–æ—Ä–∏–º —Ä–µ–∞–∫—Ü–∏—é –°–†–ê–ó–£, –Ω–µ –∂–¥—ë–º
        say(reaction) {
            DispatchQueue.main.async {
                self.statusText = "üß† –†–∞—Å–ø–æ–∑–Ω–∞—é..."
            }
        }

        // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–≤–∏–∂–æ–∫
        audioEngine?.stop()
        audioEngine?.inputNode.removeTap(onBus: 0)

        // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–µ—Ä
        silenceTimer?.invalidate()
        silenceTimer = nil

        // –ó–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª
        audioFile = nil
        audioEngine = nil

        print("‚úÖ Auto-recording stopped")
        print("========== END VAD ==========\n")

        transcribeAudio()
    }

    // –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ (–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ–≥–æ)
    func interrupt() {
        print("üõë INTERRUPT - stopping everything")

        // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø–∏—Å—å
        if isListening {
            audioEngine?.stop()
            audioEngine?.inputNode.removeTap(onBus: 0)
            audioFile = nil
            audioEngine = nil
            isListening = false
        }

        // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–µ—Ä
        silenceTimer?.invalidate()
        silenceTimer = nil

        // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ
        player?.stop()
        isSpeaking = false

        isProcessing = false
        statusText = "üõë –ü—Ä–µ—Ä–≤–∞–Ω–æ"

        // –ï—Å–ª–∏ –∞–≤—Ç–æ —Ä–µ–∂–∏–º ‚Üí –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ —Å–µ–∫—É–Ω–¥—É
        if isAutoMode {
            DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) {
                self.startListeningAuto()
            }
        }
    }

    // MARK: –û—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
    func clearHistory() {
        conversationHistory.removeAll()
        recognizedText = ""
        responseText = ""
        statusText = "üóëÔ∏è –ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞"
        print("üóëÔ∏è Conversation history cleared")

        // –ß–µ—Ä–µ–∑ —Å–µ–∫—É–Ω–¥—É –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –≤ –æ–±—ã—á–Ω—ã–π —Å—Ç–∞—Ç—É—Å
        DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) {
            if self.isAutoMode && !self.isListening && !self.isProcessing {
                self.statusText = "üëÇ –°–ª—É—à–∞—é..."
            } else if !self.isListening && !self.isProcessing {
                self.statusText = "üí§ –ñ–¥—É –∫–æ–º–∞–Ω–¥—ã"
            }
        }
    }

    // MARK: Whisper (—Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º)
    private func transcribeAudio() {
        isProcessing = true
        statusText = "üß† –†–∞—Å–ø–æ–∑–Ω–∞—é —Ä–µ—á—å..."
        print("\n========== WHISPER API ==========")

        guard let audioData = try? Data(contentsOf: audioFilename) else {
            print("‚ùå Cannot read audio file")
            DispatchQueue.main.async {
                self.statusText = "‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞"
                self.isProcessing = false
            }
            return
        }

        print("üì¶ Audio file size: \(audioData.count / 1024)KB")

        var req = URLRequest(url: URL(string: "https://api.openai.com/v1/audio/transcriptions")!)
        req.httpMethod = "POST"
        req.addValue("Bearer \(openAIKey)", forHTTPHeaderField: "Authorization")

        let boundary = "Boundary-\(UUID().uuidString)"
        req.setValue("multipart/form-data; boundary=\(boundary)", forHTTPHeaderField: "Content-Type")

        var body = Data()
        body.append("--\(boundary)\r\n".data(using: .utf8)!)
        body.append("Content-Disposition: form-data; name=\"file\"; filename=\"input.wav\"\r\n".data(using: .utf8)!)
        body.append("Content-Type: audio/wav\r\n\r\n".data(using: .utf8)!)
        body.append(audioData)

        // model
        body.append("\r\n--\(boundary)\r\n".data(using: .utf8)!)
        body.append("Content-Disposition: form-data; name=\"model\"\r\n\r\n".data(using: .utf8)!)
        body.append("whisper-1\r\n".data(using: .utf8)!)

        // —è–∑—ã–∫ ‚Äî —Ñ–∏–∫—Å–∏—Ä—É–µ–º —Ä—É—Å—Å–∫–∏–π
        body.append("--\(boundary)\r\n".data(using: .utf8)!)
        body.append("Content-Disposition: form-data; name=\"language\"\r\n\r\n".data(using: .utf8)!)
        body.append("ru\r\n".data(using: .utf8)!)

        // prompt ‚Äî –ø–æ–¥—Å–∫–∞–∑–∫–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (—É–º–µ–Ω—å—à–∞–µ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏)
        body.append("--\(boundary)\r\n".data(using: .utf8)!)
        body.append("Content-Disposition: form-data; name=\"prompt\"\r\n\r\n".data(using: .utf8)!)
        body.append("–†–∞–∑–≥–æ–≤–æ—Ä —Ä–µ–±–µ–Ω–∫–∞ —Å –≥–æ–ª–æ—Å–æ–≤—ã–º –ø–æ–º–æ—â–Ω–∏–∫–æ–º. –í–æ–ø—Ä–æ—Å—ã –ø—Ä–æ —É—á–µ–±—É, –∫–æ—Å–º–æ—Å, –∏–≥—Ä—ã.\r\n".data(using: .utf8)!)

        // temperature ‚Äî —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è (0 = –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ)
        body.append("--\(boundary)\r\n".data(using: .utf8)!)
        body.append("Content-Disposition: form-data; name=\"temperature\"\r\n\r\n".data(using: .utf8)!)
        body.append("0.0\r\n".data(using: .utf8)!)

        body.append("--\(boundary)--\r\n".data(using: .utf8)!)
        req.httpBody = body

        print("üì§ Sending to Whisper API...")
        let startTime = Date()

        URLSession.shared.dataTask(with: req) { data, response, error in
            let elapsed = Date().timeIntervalSince(startTime)
            print("‚è±Ô∏è Whisper response time: \(String(format: "%.1f", elapsed))s")

            if let error = error {
                print("‚ùå Whisper error: \(error.localizedDescription)")
                DispatchQueue.main.async {
                    self.statusText = "‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏"
                    self.isProcessing = false
                }
                return
            }

            guard let data = data else {
                print("‚ùå No data received")
                DispatchQueue.main.async {
                    self.statusText = "‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
                    self.isProcessing = false
                }
                return
            }

            guard let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any] else {
                print("‚ùå Cannot parse JSON")
                if let responseString = String(data: data, encoding: .utf8) {
                    print("Raw response: \(responseString)")
                }
                DispatchQueue.main.async {
                    self.statusText = "‚ùå –û—à–∏–±–∫–∞ API"
                    self.isProcessing = false
                }
                return
            }

            guard let text = json["text"] as? String else {
                print("‚ùå No 'text' field in response")
                print("JSON: \(json)")
                DispatchQueue.main.async {
                    self.statusText = "‚ùå –ù–µ—Ç —Ç–µ–∫—Å—Ç–∞"
                    self.isProcessing = false
                }
                return
            }

            let trimmedText = text.trimmingCharacters(in: .whitespacesAndNewlines)
            print("‚úÖ Recognized: \"\(trimmedText)\"")

            // ‚úÖ –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ú–£–°–û–†–ù–´–• –†–ê–°–ü–û–ó–ù–ê–í–ê–ù–ò–ô (Whisper –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏)
            let junkPhrases = [
                "—Ç–µ–º–∞ –∂–∏–≤–æ—Ç–Ω—ã–µ –∏ —Ñ—Ä—É–∫—Ç—ã",
                "—Å–ø–∞—Å–∏–±–æ –∑–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä",
                "–ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å –Ω–∞ –∫–∞–Ω–∞–ª",
                "—Å—Ç–∞–≤—å—Ç–µ –ª–∞–π–∫–∏",
                "—Å—É–±—Ç–∏—Ç—Ä—ã",
                "–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–ª–µ–¥—É–µ—Ç",
                "–º—É–∑—ã–∫–∞",
                "–∞–ø–ª–æ–¥–∏—Å–º–µ–Ω—Ç—ã"
            ]

            let lowercased = trimmedText.lowercased()
            let isJunk = junkPhrases.contains { lowercased.contains($0) }

            if isJunk {
                print("‚ö†Ô∏è JUNK detected - ignoring Whisper hallucination: \"\(trimmedText)\"")
                DispatchQueue.main.async {
                    self.statusText = "ü§∑ –ù–µ —Ä–∞—Å—Å–ª—ã—à–∞–ª"
                    self.isProcessing = false

                    // –í –∞–≤—Ç–æ —Ä–µ–∂–∏–º–µ ‚Üí —Å—Ä–∞–∑—É –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å–ª—É—à–∞—Ç—å
                    if self.isAutoMode {
                        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
                            self.startListeningAuto()
                        }
                    }
                }
                print("========== END WHISPER ==========\n")
                return
            }

            if trimmedText.isEmpty || trimmedText.count < 2 {
                print("‚ö†Ô∏è Too short or empty - nothing heard")
                DispatchQueue.main.async {
                    self.statusText = "ü§∑ –ù–∏—á–µ–≥–æ –Ω–µ —É—Å–ª—ã—à–∞–ª"
                    self.recognizedText = "(–ø—É—Å—Ç–æ)"
                    self.isProcessing = false

                    // –í –∞–≤—Ç–æ —Ä–µ–∂–∏–º–µ ‚Üí –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å–ª—É—à–∞—Ç—å
                    if self.isAutoMode {
                        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
                            self.startListeningAuto()
                        }
                    }
                }
                print("========== END WHISPER ==========\n")
                return
            }

            print("========== END WHISPER ==========\n")

            DispatchQueue.main.async {
                self.recognizedText = trimmedText
                self.askGPT(trimmedText)
            }
        }.resume()
    }

    // MARK: GPT (—Å –∏—Å—Ç–æ—Ä–∏–µ–π —Ä–∞–∑–≥–æ–≤–æ—Ä–∞)
    private func askGPT(_ text: String) {
        guard !text.isEmpty else {
            print("‚ö†Ô∏è Empty text for GPT")
            DispatchQueue.main.async {
                self.isProcessing = false
                self.statusText = "üí§ –ñ–¥—É –∫–æ–º–∞–Ω–¥—ã"
            }
            return
        }

        // –†–µ–∞–∫—Ü–∏—è —É–∂–µ –±—ã–ª–∞ —Å–∫–∞–∑–∞–Ω–∞ –≤ stopListeningAuto()
        // –°—Ä–∞–∑—É –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å "–¥—É–º–∞—é"
        DispatchQueue.main.async {
            self.statusText = "ü§î –î—É–º–∞—é..."
        }

        print("\n========== GPT API ==========")
        print("üìù User input: \"\(text)\"")

        let url = URL(string: "https://api.openai.com/v1/chat/completions")!

        // ‚úÖ –°–û–ö–†–ê–©–ï–ù–ù–´–ô –ü–†–û–ú–ü–¢ (—ç–∫–æ–Ω–æ–º–∏—è —Ç–æ–∫–µ–Ω–æ–≤, –±—ã—Å—Ç—Ä–µ–µ –æ–±—Ä–∞–±–æ—Ç–∫–∞)
        let systemPrompt = """
        –¢—ã –ú–∞–ª–æ–π ‚Äî –≥–æ–ª–æ—Å–æ–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫. –ì–æ–≤–æ—Ä–∏—à—å —Å –§—ë–¥–æ—Ä–æ–º (15 –ª–µ—Ç, –Ω–µ–∑—Ä—è—á–∏–π, 8 –∫–ª–∞—Å—Å).
        –°—Ç–∏–ª—å: –Ω–∞ —Ä–∞–≤–Ω—ã—Ö, –±–µ–∑ —Å—é—Å—é–∫–∞–Ω—å—è, —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å–ª–µ–Ω–≥ OK, –º–æ–∂–Ω–æ —à—É—Ç–∏—Ç—å.
        –û–ø–∏—Å—ã–≤–∞—è –ø—Ä–µ–¥–º–µ—Ç—ã ‚Üí —É–ø–æ–º–∏–Ω–∞–π —Ñ–æ—Ä–º—É, —Ä–∞–∑–º–µ—Ä, —Ç–µ–∫—Å—Ç—É—Ä—É (–æ–Ω –Ω–µ–∑—Ä—è—á–∏–π).
        –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ (2-4 —Ñ—Ä–∞–∑—ã). –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å–ª–æ–≤–∞ –º–æ–∂–Ω–æ.
        """

        // ‚úÖ –î–û–ë–ê–í–õ–Ø–ï–ú –¢–ï–ö–£–©–ò–ô –í–û–ü–†–û–° –í –ò–°–¢–û–†–ò–Æ
        conversationHistory.append(["role": "user", "content": text])

        // ‚úÖ –°–ö–û–õ–¨–ó–Ø–©–ï–ï –û–ö–ù–û: —É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –ø–∞—Ä—ã, –µ—Å–ª–∏ –∏—Å—Ç–æ—Ä–∏—è —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è
        // –ö–∞–∂–¥–∞—è –ø–∞—Ä–∞ = user + assistant = 2 —Å–æ–æ–±—â–µ–Ω–∏—è
        // –•—Ä–∞–Ω–∏–º maxHistoryPairs * 2 = 8 —Å–æ–æ–±—â–µ–Ω–∏–π (4 –ø–∞—Ä—ã)
        while conversationHistory.count > maxHistoryPairs * 2 {
            conversationHistory.removeFirst(2)  // –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä–µ–π—à—É—é –ø–∞—Ä—É (user + assistant)
            print("üóëÔ∏è Removed oldest conversation pair (sliding window)")
        }

        // ‚úÖ –§–û–†–ú–ò–†–£–ï–ú MESSAGES: —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç + –≤—Å—è –∏—Å—Ç–æ—Ä–∏—è
        var messages: [[String: String]] = [
            ["role": "system", "content": systemPrompt]
        ]
        messages.append(contentsOf: conversationHistory)

        print("üìö Conversation history: \(conversationHistory.count) messages (\(conversationHistory.count / 2) pairs)")

        let body: [String: Any] = [
            "model": "gpt-3.5-turbo",
            "messages": messages,
            "max_tokens": 150
        ]

        var req = URLRequest(url: url)
        req.httpMethod = "POST"
        req.addValue("Bearer \(openAIKey)", forHTTPHeaderField: "Authorization")
        req.addValue("application/json", forHTTPHeaderField: "Content-Type")
        req.httpBody = try? JSONSerialization.data(withJSONObject: body)

        print("üì§ Sending to GPT...")
        let startTime = Date()

        URLSession.shared.dataTask(with: req) { data, response, error in
            let elapsed = Date().timeIntervalSince(startTime)
            print("‚è±Ô∏è GPT response time: \(String(format: "%.1f", elapsed))s")

            if let error = error {
                print("‚ùå GPT error: \(error.localizedDescription)")
                DispatchQueue.main.async {
                    self.statusText = "‚ùå –û—à–∏–±–∫–∞ GPT"
                    self.isProcessing = false
                }
                return
            }

            guard let data = data,
                  let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any] else {
                print("‚ùå Cannot parse GPT response")
                DispatchQueue.main.async {
                    self.statusText = "‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞"
                    self.isProcessing = false
                }
                return
            }

            guard let choices = json["choices"] as? [[String: Any]],
                  let msg = choices.first?["message"] as? [String: Any],
                  let reply = msg["content"] as? String else {
                print("‚ùå No content in GPT response")
                print("JSON: \(json)")
                DispatchQueue.main.async {
                    self.statusText = "‚ùå –ù–µ—Ç –æ—Ç–≤–µ—Ç–∞"
                    self.isProcessing = false
                }
                return
            }

            print("‚úÖ GPT reply: \"\(reply)\"")

            // ‚úÖ –î–û–ë–ê–í–õ–Ø–ï–ú –û–¢–í–ï–¢ GPT –í –ò–°–¢–û–†–ò–Æ
            self.conversationHistory.append(["role": "assistant", "content": reply])
            print("üìö Added assistant response to history (now \(self.conversationHistory.count) messages)")
            print("========== END GPT ==========\n")

            // –ú–û–ú–ï–ù–¢–ê–õ–¨–ù–û –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
            DispatchQueue.main.async {
                self.responseText = reply
                self.statusText = "üó£Ô∏è –ì–æ—Ç–æ–≤–ª—é –æ–∑–≤—É—á–∫—É..."
            }

            // TTS –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ (–Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç UI)
            self.say(reply) {
                DispatchQueue.main.async {
                    self.isProcessing = false
                    // –ï—Å–ª–∏ –∞–≤—Ç–æ —Ä–µ–∂–∏–º ‚Üí –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å–ª—É—à–∞—Ç—å
                    if self.isAutoMode {
                        self.startListeningAuto()
                    } else {
                        self.statusText = "üí§ –ñ–¥—É –∫–æ–º–∞–Ω–¥—ã"
                    }
                }
            }
        }.resume()
    }

    // MARK: TTS (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
    func say(_ text: String, completion: (() -> Void)? = nil) {
        print("\n========== TTS API ==========")
        print("üí¨ Text to speak: \"\(text)\"")

        // –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å —Ç–æ–ª—å–∫–æ –≤ main thread
        DispatchQueue.main.async {
            self.statusText = "üó£Ô∏è –ì–æ–≤–æ—Ä—é..."
        }

        guard let url = URL(string: "https://api.openai.com/v1/audio/speech") else {
            print("‚ùå Invalid TTS URL")
            completion?()
            return
        }

        let json: [String: Any] = [
            "model": "gpt-4o-mini-tts",
            "voice": "alloy",
            "input": text,
            "speed": 1.15  // –£–≤–µ–ª–∏—á–∏–ª–∏ —Å–∫–æ—Ä–æ—Å—Ç—å —Å 1.0 –¥–æ 1.15 –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ä–µ—á–∏
        ]

        var req = URLRequest(url: url)
        req.httpMethod = "POST"
        req.addValue("Bearer \(openAIKey)", forHTTPHeaderField: "Authorization")
        req.addValue("application/json", forHTTPHeaderField: "Content-Type")
        req.httpBody = try? JSONSerialization.data(withJSONObject: json)

        print("üì§ Requesting TTS...")
        let startTime = Date()

        URLSession.shared.dataTask(with: req) { data, response, error in
            let elapsed = Date().timeIntervalSince(startTime)
            print("‚è±Ô∏è TTS response time: \(String(format: "%.1f", elapsed))s")

            if let error = error {
                print("‚ùå TTS error:", error.localizedDescription)
                DispatchQueue.main.async {
                    self.statusText = "‚ùå –û—à–∏–±–∫–∞ TTS"
                }
                completion?()
                return
            }

            guard let data = data, !data.isEmpty else {
                print("‚ùå Empty TTS response")
                DispatchQueue.main.async {
                    self.statusText = "‚ùå –ù–µ—Ç –∞—É–¥–∏–æ"
                }
                completion?()
                return
            }

            print("‚úÖ TTS received (\(data.count / 1024)KB)")

            let tmp = FileManager.default.temporaryDirectory.appendingPathComponent("speech.mp3")
            do {
                try data.write(to: tmp, options: .atomic)
                print("‚úÖ TTS file saved: \(tmp.path)")
                DispatchQueue.main.async {
                    self.playAudio(from: tmp, completion: completion)
                }
            } catch {
                print("‚ùå TTS write error:", error)
                completion?()
            }
        }.resume()
    }

    private func playAudio(from url: URL, completion: (() -> Void)? = nil) {
        print("üîä Starting playback...")

        do {
            // –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –ø–µ—Ä–µ–¥ TTS
            try AVAudioSession.sharedInstance().setCategory(.playback, mode: .default)
            try AVAudioSession.sharedInstance().setActive(true)
            print("‚úÖ Audio session configured for playback")

            isSpeaking = true
            let p = try AVAudioPlayer(contentsOf: url)
            player = p

            p.prepareToPlay()
            let success = p.play()

            if !success {
                print("‚ùå Failed to start audio playback")
                self.isSpeaking = false
                DispatchQueue.main.async {
                    self.statusText = "‚ùå –û—à–∏–±–∫–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è"
                }
                completion?()
                return
            }

            print("üîä Playing audio (duration: \(String(format: "%.1f", p.duration))s)")

            // –ñ–¥–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è + –Ω–µ–±–æ–ª—å—à–æ–π –±—É—Ñ–µ—Ä
            DispatchQueue.main.asyncAfter(deadline: .now() + p.duration + 0.3) {
                print("‚úÖ Playback finished")
                print("========== END TTS ==========\n")
                self.isSpeaking = false
                completion?()
            }
        } catch {
            print("‚ùå TTS play error:", error)
            self.isSpeaking = false
            DispatchQueue.main.async {
                self.statusText = "‚ùå –û—à–∏–±–∫–∞ –ø–ª–µ–µ—Ä–∞"
            }
            completion?()
        }
    }
}
