import SwiftUI
import AVFoundation
import Combine

struct ContentView: View {
    @StateObject private var audioManager = AudioManager()
    
    var body: some View {
        VStack(spacing: 20) {
            Text(audioManager.isListening ? "üéôÔ∏è –ú–∞–ª–æ–π —Å–ª—É—à–∞–µ—Ç..." : "ü§ñ –ú–∞–ª–æ–π –≥–æ–≤–æ—Ä–∏—Ç...")
                .font(.title2).bold()
                .padding()
            
            if !audioManager.recognizedText.isEmpty {
                Text("üëÇ \(audioManager.recognizedText)")
                    .foregroundColor(.gray)
                    .padding()
            }
            
            if !audioManager.responseText.isEmpty {
                Text("üí¨ \(audioManager.responseText)")
                    .padding()
            }
        }
        .padding()
        .onAppear { audioManager.startConversation() }
    }
}

// MARK: - AUDIO MANAGER

final class AudioManager: NSObject, ObservableObject {
    @Published var recognizedText = ""
    @Published var responseText = ""
    @Published var isListening = false

    // API key is stored in Config.swift (not tracked in git for security)
    private let openAIKey = Config.openAIKey
    
    private let audioFilename = FileManager.default.temporaryDirectory.appendingPathComponent("input.wav")
    private var audioEngine: AVAudioEngine?
    private var audioFile: AVAudioFile?
    private var silenceTimer: Timer?
    private var lastSpeechTime = Date()
    private var player: AVAudioPlayer?
    private var isProcessing = false
    private var isSpeaking = false
    private var lastRecognizedText = "" // –î–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–æ–≤

    // MARK: –°—Ç–∞—Ä—Ç –¥–∏–∞–ª–æ–≥–∞
    func startConversation() {
        say("–ü—Ä–∏–≤–µ—Ç, —è –ú–∞–ª–æ–π! –ß–µ–º –∑–∞–π–º—ë–º—Å—è, –§—ë–¥–æ—Ä?") {
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.8) {
                self.startListening()
            }
        }
    }

    // MARK: –°–ª—É—à–∞–Ω–∏–µ (–∑–∞–ø–∏—Å—å –≤ —Ä–æ–¥–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞)
    func startListening() {
        guard !isSpeaking else { return }
        recognizedText = ""
        isListening = true
        isProcessing = false
        lastSpeechTime = Date()

        // –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –∑–∞–ø–∏—Å—å –ø–µ—Ä–µ–¥ —Å—Ç–∞—Ä—Ç–æ–º –¥–≤–∏–∂–∫–∞
        do {
            try AVAudioSession.sharedInstance().setCategory(.record, mode: .default, options: [.allowBluetoothHFP])
            try AVAudioSession.sharedInstance().setActive(true)
        } catch {
            print("Audio session record error:", error)
        }

        let engine = AVAudioEngine()
        audioEngine = engine
        let input = engine.inputNode

        // –ß–∏—Å—Ç–∏–º –ø—Ä–µ–∂–Ω–∏–π tap
        input.removeTap(onBus: 0)

        // –ü–æ–ª—É—á–∞–µ–º —Ñ–æ—Ä–º–∞—Ç –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å –ª—é–±–æ–π - –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π, –Ω–∞—É—à–Ω–∏–∫–∏, bluetooth)
        let inputFormat = input.outputFormat(forBus: 0)

        print("üé§ Recording format: \(inputFormat.sampleRate)Hz, \(inputFormat.channelCount) ch")

        // –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è tap (–±–µ–∑ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏)
        do {
            audioFile = try AVAudioFile(forWriting: audioFilename,
                                        settings: inputFormat.settings,
                                        commonFormat: .pcmFormatFloat32,
                                        interleaved: false)
        } catch {
            print("‚ö†Ô∏è Audio file error:", error)
            return
        }

        // Tap –≤ —Ä–æ–¥–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤–µ–∑–¥–µ)
        input.installTap(onBus: 0, bufferSize: 4096, format: inputFormat) { buffer, _ in
            // –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º —Ä–µ—á—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏—à–∏–Ω—ã
            self.detectSpeech(buffer: buffer)

            // –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –±—É—Ñ–µ—Ä –∫–∞–∫ –µ—Å—Ç—å
            do {
                try self.audioFile?.write(from: buffer)
            } catch {
                print("‚ö†Ô∏è File write error:", error)
            }
        }

        do {
            try engine.start()
        } catch {
            print("Engine start error:", error)
            return
        }

        startSilenceTimer()
        print("üéß Listening started in native format")
    }

    // –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–ª—É—à–∞–Ω–∏—è (–¥–µ—Ä–≥–∞–µ—Ç—Å—è —Ç–∞–π–º–µ—Ä–æ–º —Ç–∏—à–∏–Ω—ã)
    func stopListening() {
        isListening = false
        silenceTimer?.invalidate()
        audioEngine?.stop()
        audioEngine?.inputNode.removeTap(onBus: 0)
        print("üõë Listening stopped")
        transcribeAudio()
    }

    // MARK: –î–µ—Ç–µ–∫—Ç–æ—Ä —Ä–µ—á–∏ (—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)
    private func detectSpeech(buffer: AVAudioPCMBuffer) {
        guard !isProcessing, let channel = buffer.floatChannelData?[0] else { return }
        let count = Int(buffer.frameLength)
        let frame = Array(UnsafeBufferPointer(start: channel, count: count))
        let mean = frame.map { $0 * $0 }.reduce(0, +) / Float(max(count, 1))
        let rms = sqrt(mean)
        let avgPower = 20 * log10(max(rms, 1e-7)) // –∑–∞—â–∏—Ç–∞ –æ—Ç -inf

        // –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥: -40dB (–º–µ–∂–¥—É —Å—Ç–∞—Ä—ã–º -45 –∏ -35)
        // –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –¥–ª—è —Ä–µ—á–∏, –Ω–æ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç —Ç–∏—Ö–∏–π —Ñ–æ–Ω–æ–≤—ã–π —à—É–º
        if avgPower > -40 {
            lastSpeechTime = Date()
        }
    }

    private func startSilenceTimer() {
        silenceTimer?.invalidate()
        silenceTimer = Timer.scheduledTimer(withTimeInterval: 0.15, repeats: true) { _ in
            // –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –¥–∏–∞–ª–æ–≥–∞: 1.3 —Å–µ–∫
            // –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –ø–∞—É–∑ –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏, –Ω–æ –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ
            if Date().timeIntervalSince(self.lastSpeechTime) > 1.3 {
                self.stopListening()
            }
        }
    }

    // MARK: Whisper
    private func transcribeAudio() {
        isProcessing = true
        print("üß† Whisper processing‚Ä¶")
        
        var req = URLRequest(url: URL(string: "https://api.openai.com/v1/audio/transcriptions")!)
        req.httpMethod = "POST"
        req.addValue("Bearer \(openAIKey)", forHTTPHeaderField: "Authorization")

        let boundary = "Boundary-\(UUID().uuidString)"
        req.setValue("multipart/form-data; boundary=\(boundary)", forHTTPHeaderField: "Content-Type")

        var body = Data()
        body.append("--\(boundary)\r\n".data(using: .utf8)!)
        body.append("Content-Disposition: form-data; name=\"file\"; filename=\"input.wav\"\r\n".data(using: .utf8)!)
        body.append("Content-Type: audio/wav\r\n\r\n".data(using: .utf8)!)
        body.append((try? Data(contentsOf: audioFilename)) ?? Data())

        // model
        body.append("\r\n--\(boundary)\r\n".data(using: .utf8)!)
        body.append("Content-Disposition: form-data; name=\"model\"\r\n\r\n".data(using: .utf8)!)
        body.append("whisper-1\r\n".data(using: .utf8)!)

        // —è–∑—ã–∫ ‚Äî —Ñ–∏–∫—Å–∏—Ä—É–µ–º —Ä—É—Å—Å–∫–∏–π
        body.append("--\(boundary)\r\n".data(using: .utf8)!)
        body.append("Content-Disposition: form-data; name=\"language\"\r\n\r\n".data(using: .utf8)!)
        body.append("ru\r\n".data(using: .utf8)!)

        body.append("--\(boundary)--\r\n".data(using: .utf8)!)
        req.httpBody = body

        URLSession.shared.dataTask(with: req) { data, _, _ in
            guard let data = data,
                  let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any],
                  let text = json["text"] as? String else {
                print("‚ö†Ô∏è Whisper error or no response")
                DispatchQueue.main.async {
                    self.isProcessing = false
                    // –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω—ã–º —Å–ª—É—à–∞–Ω–∏–µ–º –ø—Ä–∏ –æ—à–∏–±–∫–µ
                    DispatchQueue.main.asyncAfter(deadline: .now() + 0.8) {
                        self.startListening()
                    }
                }
                return
            }

            let trimmedText = text.trimmingCharacters(in: .whitespacesAndNewlines)

            // –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ - –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å–ª—É—à–∞—Ç—å
            if trimmedText.isEmpty || trimmedText.count < 2 {
                print("ü§∑ –ü—É—Å—Ç–æ –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å–ª—É—à–∞—Ç—å")
                DispatchQueue.main.async {
                    self.isProcessing = false
                    self.startListening()
                }
                return
            }

            // –ó–∞—â–∏—Ç–∞ –æ—Ç –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è —Ç–æ–≥–æ –∂–µ —Å–∞–º–æ–≥–æ (—ç—Ö–æ –∏–ª–∏ –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏–µ)
            if trimmedText == self.lastRecognizedText {
                print("‚ö†Ô∏è –ü–æ–≤—Ç–æ—Ä –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º")
                DispatchQueue.main.async {
                    self.isProcessing = false
                    self.startListening()
                }
                return
            }

            DispatchQueue.main.async {
                self.lastRecognizedText = trimmedText
                self.recognizedText = trimmedText
                print("üó£Ô∏è –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ:", self.recognizedText)
                self.askGPT(self.recognizedText)
            }
        }.resume()
    }

    // MARK: GPT (–ú–∞–ª–æ–π)
    private func askGPT(_ text: String) {
        guard !text.isEmpty else {
            self.isProcessing = false
            self.startListening()
            return
        }

        let url = URL(string: "https://api.openai.com/v1/chat/completions")!
        let systemPrompt = """
        –¢—ã –≥–æ–ª–æ—Å–æ–≤–æ–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –∏–º–µ–Ω–∏ –ú–∞–ª–æ–π. –¢—ã –∫—Ä—É—Ç–æ–π –∏ –≤–µ—Å—ë–ª—ã–π.
        –†–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞–π —Å —Ä–µ–±—ë–Ω–∫–æ–º 5 –∫–ª–∞—Å—Å–∞ –ø—Ä–æ—Å—Ç–æ, –¥—Ä—É–∂–µ–ª—é–±–Ω–æ –∏ –∫–æ—Ä–æ—Ç–∫–æ.
        –†–µ–±—ë–Ω–∫–∞ –∑–æ–≤—É—Ç –§—ë–¥–æ—Ä. –û–Ω –Ω–µ–∑—Ä—è—á–∏–π. –ü–æ—ç—Ç–æ–º—É –æ–ø–∏—Å—ã–≤–∞–π –ø—Ä–µ–¥–º–µ—Ç—ã —Ç–∞–∫—Ç–∏–ª—å–Ω–æ ‚Äî —Ñ–æ—Ä–º—É, —Ä–∞–∑–º–µ—Ä, –æ—â—É—â–µ–Ω–∏—è.
        –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –¥–ª–∏–Ω–Ω—ã–π ‚Äî –æ—Ç–≤–µ—á–∞–π —á—É—Ç—å –ø–æ–¥—Ä–æ–±–Ω–µ–µ.
        –ï—Å–ª–∏ –ø—Ä–æ—Å—è—Ç –ø–æ—á–∏—Ç–∞—Ç—å –∫–Ω–∏–≥—É ‚Äî —á–∏—Ç–∞–π —á–∞—Å—Ç—è–º–∏ (–ø–æ 3‚Äì4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).
        –ì–æ–≤–æ—Ä–∏ –ø–æ-—Ä—É—Å—Å–∫–∏. –ò–Ω–æ–≥–¥–∞ –≤—Å—Ç–∞–≤–ª—è–π –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è —Å –∫–æ—Ä–æ—Ç–∫–∏–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º.
        –û—Ç–≤–µ—á–∞–π –Ω–µ –¥–æ–ª—å—à–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.
        """

        let body: [String: Any] = [
            "model": "gpt-4o-mini",
            "messages": [
                ["role": "system", "content": systemPrompt],
                ["role": "user", "content": text]
            ],
            "max_tokens": 200
        ]

        var req = URLRequest(url: url)
        req.httpMethod = "POST"
        req.addValue("Bearer \(openAIKey)", forHTTPHeaderField: "Authorization")
        req.addValue("application/json", forHTTPHeaderField: "Content-Type")
        req.httpBody = try? JSONSerialization.data(withJSONObject: body)

        print("ü§ñ Asking GPT...")

        URLSession.shared.dataTask(with: req) { data, _, _ in
            guard let data = data,
                  let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any],
                  let choices = json["choices"] as? [[String: Any]],
                  let msg = choices.first?["message"] as? [String: Any],
                  let reply = msg["content"] as? String else {
                print("‚ùå GPT error")
                DispatchQueue.main.async {
                    self.isProcessing = false
                    self.startListening()
                }
                return
            }

            print("‚úÖ GPT response received")

            // –°—Ä–∞–∑—É –æ–±–Ω–æ–≤–ª—è–µ–º UI –∏ –∑–∞–ø—É—Å–∫–∞–µ–º TTS (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ, –±–µ–∑ –æ–∂–∏–¥–∞–Ω–∏—è)
            DispatchQueue.main.async {
                self.responseText = reply
                print("üí¨ –ú–∞–ª–æ–π:", reply)

                // TTS –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ù–ï–ú–ï–î–õ–ï–ù–ù–û –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
                self.say(reply) {
                    self.isProcessing = false
                    self.startListening()
                }
            }
        }.resume()
    }

    // MARK: OpenAI TTS (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞)
    func say(_ text: String, completion: (() -> Void)? = nil) {
        guard let url = URL(string: "https://api.openai.com/v1/audio/speech") else { return }
        let json: [String: Any] = [
            "model": "gpt-4o-mini-tts",
            "voice": "alloy", // –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å "verse", "shimmer", "soft"
            "input": text,
            "speed": 1.0 // –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
        ]

        var req = URLRequest(url: url)
        req.httpMethod = "POST"
        req.addValue("Bearer \(openAIKey)", forHTTPHeaderField: "Authorization")
        req.addValue("application/json", forHTTPHeaderField: "Content-Type")
        req.httpBody = try? JSONSerialization.data(withJSONObject: json)

        print("üéµ Requesting TTS...")

        URLSession.shared.dataTask(with: req) { data, response, error in
            if let error = error {
                print("‚ùå TTS error:", error.localizedDescription)
                completion?()
                return
            }

            guard let data = data, !data.isEmpty else {
                print("‚ùå Empty TTS response")
                completion?()
                return
            }

            let tmp = FileManager.default.temporaryDirectory.appendingPathComponent("speech.mp3")
            do {
                try data.write(to: tmp, options: .atomic)
                print("‚úÖ TTS received (\(data.count / 1024)KB)")
                DispatchQueue.main.async {
                    self.playAudio(from: tmp, completion: completion)
                }
            } catch {
                print("‚ùå TTS write error:", error)
                completion?()
            }
        }.resume()
    }

    private func playAudio(from url: URL, completion: (() -> Void)? = nil) {
        do {
            // –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –ø–µ—Ä–µ–¥ TTS
            try AVAudioSession.sharedInstance().setCategory(.playback, mode: .default)
            try AVAudioSession.sharedInstance().setActive(true)

            isSpeaking = true
            let p = try AVAudioPlayer(contentsOf: url)
            player = p

            // –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—é (–∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤ –ø–∞–º—è—Ç—å)
            p.prepareToPlay()

            // –ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ–µ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ
            let success = p.play()

            if !success {
                print("‚ö†Ô∏è Failed to start audio playback")
                self.isSpeaking = false
                completion?()
                return
            }

            print("üîä Playing audio (\(String(format: "%.1f", p.duration))s)")

            // –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π —Ç–∞–π–º–µ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            DispatchQueue.main.asyncAfter(deadline: .now() + p.duration + 0.3) {
                self.isSpeaking = false
                completion?()
            }
        } catch {
            print("TTS play error:", error)
            self.isSpeaking = false
            completion?()
        }
    }
}
