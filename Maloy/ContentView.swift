import SwiftUI
import AVFoundation
import Combine

struct ContentView: View {
    @StateObject private var audioManager = AudioManager()
    
    var body: some View {
        VStack(spacing: 20) {
            Text(audioManager.isListening ? "üéôÔ∏è –ú–∞–ª–æ–π —Å–ª—É—à–∞–µ—Ç..." : "ü§ñ –ú–∞–ª–æ–π –≥–æ–≤–æ—Ä–∏—Ç...")
                .font(.title2).bold()
                .padding()
            
            if !audioManager.recognizedText.isEmpty {
                Text("üëÇ \(audioManager.recognizedText)")
                    .foregroundColor(.gray)
                    .padding()
            }
            
            if !audioManager.responseText.isEmpty {
                Text("üí¨ \(audioManager.responseText)")
                    .padding()
            }
        }
        .padding()
        .onAppear { audioManager.startConversation() }
    }
}

// MARK: - AUDIO MANAGER

final class AudioManager: NSObject, ObservableObject {
    @Published var recognizedText = ""
    @Published var responseText = ""
    @Published var isListening = false

    // API key is stored in Config.swift (not tracked in git for security)
    private let openAIKey = Config.openAIKey
    
    private let audioFilename = FileManager.default.temporaryDirectory.appendingPathComponent("input.wav")
    private var audioEngine: AVAudioEngine?
    private var audioFile: AVAudioFile?
    private var silenceTimer: Timer?
    private var lastSpeechTime = Date()
    private var player: AVAudioPlayer?
    private var isProcessing = false
    private var isSpeaking = false

    // MARK: –°—Ç–∞—Ä—Ç –¥–∏–∞–ª–æ–≥–∞
    func startConversation() {
        say("–ü—Ä–∏–≤–µ—Ç, —è –ú–∞–ª–æ–π! –ß–µ–º –∑–∞–π–º—ë–º—Å—è, –§—ë–¥–æ—Ä?") {
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.8) {
                self.startListening()
            }
        }
    }

    // MARK: –°–ª—É—à–∞–Ω–∏–µ (tap –≤ —Ä–æ–¥–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ + –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ 16kHz/mono Int16)
    func startListening() {
        guard !isSpeaking else { return }
        recognizedText = ""
        isListening = true
        isProcessing = false
        lastSpeechTime = Date()

        // –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –∑–∞–ø–∏—Å—å –ø–µ—Ä–µ–¥ —Å—Ç–∞—Ä—Ç–æ–º –¥–≤–∏–∂–∫–∞
        do {
            try AVAudioSession.sharedInstance().setCategory(.record, mode: .default, options: [.allowBluetoothHFP])
            try AVAudioSession.sharedInstance().setActive(true)
        } catch {
            print("Audio session record error:", error)
        }

        let engine = AVAudioEngine()
        audioEngine = engine
        let input = engine.inputNode

        // –†–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ (–≤–∞–∂–Ω–æ: inputFormat, –∞ –Ω–µ outputFormat)
        let inputFormat = input.inputFormat(forBus: 0)

        // –¶–µ–ª–µ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è Whisper
        let desiredFormat = AVAudioFormat(commonFormat: .pcmFormatInt16,
                                          sampleRate: 16_000,
                                          channels: 1,
                                          interleaved: true)!
        guard let converter = AVAudioConverter(from: inputFormat, to: desiredFormat) else {
            print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å AVAudioConverter"); return
        }

        do {
            audioFile = try AVAudioFile(forWriting: audioFilename, settings: desiredFormat.settings)
        } catch {
            print("Audio file error:", error)
            return
        }

        // –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π ‚Äî —á–∏—Å—Ç–∏–º –ø—Ä–µ–∂–Ω–∏–π tap
        input.removeTap(onBus: 0)

        // Tap —Å—Ç–∞–≤–∏–º —Å format: nil (–ø—É—Å—Ç—å —Å–∏—Å—Ç–µ–º–∞ –æ—Ç–¥–∞—ë—Ç —Ä–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç)
        input.installTap(onBus: 0, bufferSize: 1024, format: nil) { buffer, _ in
            guard let converted = AVAudioPCMBuffer(pcmFormat: desiredFormat,
                                                   frameCapacity: AVAudioFrameCount(1024)) else { return }

            var error: NSError?
            converter.convert(to: converted, error: &error) { _, outStatus in
                outStatus.pointee = .haveData
                return buffer
            }
            if let e = error {
                print("Conversion error:", e)
                return
            }

            // ‚úÖ –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ Float32 (—á—Ç–æ–±—ã —Ç–æ—á–Ω–æ –∑–∞–ø–∏—Å—ã–≤–∞–ª–æ—Å—å –±–µ–∑ –ø–∞–¥–µ–Ω–∏–π)
            guard let floatFormat = AVAudioFormat(commonFormat: .pcmFormatFloat32,
                                                  sampleRate: converted.format.sampleRate,
                                                  channels: converted.format.channelCount,
                                                  interleaved: false),
                  let floatBuffer = AVAudioPCMBuffer(pcmFormat: floatFormat,
                                                     frameCapacity: converted.frameCapacity)
            else { return }

            floatBuffer.frameLength = converted.frameLength
            for c in 0..<Int(converted.format.channelCount) {
                let src = converted.int16ChannelData![c]
                let dst = floatBuffer.floatChannelData![c]
                let count = Int(converted.frameLength)
                for i in 0..<count {
                    dst[i] = Float(src[i]) / Float(Int16.max)
                }
            }

            self.detectSpeech(buffer: floatBuffer)

            do {
                try self.audioFile?.write(from: floatBuffer)
            } catch {
                print("‚ö†Ô∏è File write error:", error)
            }
        }

        do {
            try engine.start()
        } catch {
            print("Engine start error:", error)
            return
        }

        startSilenceTimer()
        print("üéß Listening started (mono 16kHz)")
    }

    // –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–ª—É—à–∞–Ω–∏—è (–¥–µ—Ä–≥–∞–µ—Ç—Å—è —Ç–∞–π–º–µ—Ä–æ–º —Ç–∏—à–∏–Ω—ã)
    func stopListening() {
        isListening = false
        silenceTimer?.invalidate()
        audioEngine?.stop()
        audioEngine?.inputNode.removeTap(onBus: 0)
        print("üõë Listening stopped")
        transcribeAudio()
    }

    // MARK: –î–µ—Ç–µ–∫—Ç–æ—Ä —Ä–µ—á–∏ (–ø—Ä–æ—Å—Ç–∞—è RMS-–æ—Ü–µ–Ω–∫–∞)
    private func detectSpeech(buffer: AVAudioPCMBuffer) {
        guard !isProcessing, let channel = buffer.floatChannelData?[0] else { return }
        let count = Int(buffer.frameLength)
        let frame = Array(UnsafeBufferPointer(start: channel, count: count))
        let mean = frame.map { $0 * $0 }.reduce(0, +) / Float(max(count, 1))
        let rms = sqrt(mean)
        let avgPower = 20 * log10(max(rms, 1e-7)) // –∑–∞—â–∏—Ç–∞ –æ—Ç -inf
        if avgPower > -45 { lastSpeechTime = Date() }
    }

    private func startSilenceTimer() {
        silenceTimer?.invalidate()
        silenceTimer = Timer.scheduledTimer(withTimeInterval: 0.3, repeats: true) { _ in
            if Date().timeIntervalSince(self.lastSpeechTime) > 2.0 {
                self.stopListening()
            }
        }
    }

    // MARK: Whisper
    private func transcribeAudio() {
        isProcessing = true
        print("üß† Whisper processing‚Ä¶")
        
        var req = URLRequest(url: URL(string: "https://api.openai.com/v1/audio/transcriptions")!)
        req.httpMethod = "POST"
        req.addValue("Bearer \(openAIKey)", forHTTPHeaderField: "Authorization")

        let boundary = "Boundary-\(UUID().uuidString)"
        req.setValue("multipart/form-data; boundary=\(boundary)", forHTTPHeaderField: "Content-Type")

        var body = Data()
        body.append("--\(boundary)\r\n".data(using: .utf8)!)
        body.append("Content-Disposition: form-data; name=\"file\"; filename=\"input.wav\"\r\n".data(using: .utf8)!)
        body.append("Content-Type: audio/wav\r\n\r\n".data(using: .utf8)!)
        body.append((try? Data(contentsOf: audioFilename)) ?? Data())

        // model
        body.append("\r\n--\(boundary)\r\n".data(using: .utf8)!)
        body.append("Content-Disposition: form-data; name=\"model\"\r\n\r\n".data(using: .utf8)!)
        body.append("whisper-1\r\n".data(using: .utf8)!)

        // —è–∑—ã–∫ ‚Äî —Ñ–∏–∫—Å–∏—Ä—É–µ–º —Ä—É—Å—Å–∫–∏–π
        body.append("--\(boundary)\r\n".data(using: .utf8)!)
        body.append("Content-Disposition: form-data; name=\"language\"\r\n\r\n".data(using: .utf8)!)
        body.append("ru\r\n".data(using: .utf8)!)

        body.append("--\(boundary)--\r\n".data(using: .utf8)!)
        req.httpBody = body

        URLSession.shared.dataTask(with: req) { data, _, _ in
            guard let data = data,
                  let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any],
                  let text = json["text"] as? String else {
                DispatchQueue.main.async {
                    self.isProcessing = false
                    self.startListening()
                }
                return
            }
            DispatchQueue.main.async {
                self.recognizedText = text.trimmingCharacters(in: .whitespacesAndNewlines)
                print("üó£Ô∏è –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ:", self.recognizedText)
                self.askGPT(self.recognizedText)
            }
        }.resume()
    }

    // MARK: GPT (–ú–∞–ª–æ–π)
    private func askGPT(_ text: String) {
        guard !text.isEmpty else {
            self.isProcessing = false
            self.startListening()
            return
        }

        let url = URL(string: "https://api.openai.com/v1/chat/completions")!
        let systemPrompt = """
        –¢—ã –≥–æ–ª–æ—Å–æ–≤–æ–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –∏–º–µ–Ω–∏ –ú–∞–ª–æ–π. –¢—ã –∫—Ä—É—Ç–æ–π –∏ –≤–µ—Å—ë–ª—ã–π.
        –†–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞–π —Å —Ä–µ–±—ë–Ω–∫–æ–º 5 –∫–ª–∞—Å—Å–∞ –ø—Ä–æ—Å—Ç–æ, –¥—Ä—É–∂–µ–ª—é–±–Ω–æ –∏ –∫–æ—Ä–æ—Ç–∫–æ.
        –†–µ–±—ë–Ω–∫–∞ –∑–æ–≤—É—Ç –§—ë–¥–æ—Ä. –û–Ω –Ω–µ–∑—Ä—è—á–∏–π. –ü–æ—ç—Ç–æ–º—É –æ–ø–∏—Å—ã–≤–∞–π –ø—Ä–µ–¥–º–µ—Ç—ã —Ç–∞–∫—Ç–∏–ª—å–Ω–æ ‚Äî —Ñ–æ—Ä–º—É, —Ä–∞–∑–º–µ—Ä, –æ—â—É—â–µ–Ω–∏—è.
        –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –¥–ª–∏–Ω–Ω—ã–π ‚Äî –æ—Ç–≤–µ—á–∞–π —á—É—Ç—å –ø–æ–¥—Ä–æ–±–Ω–µ–µ.
        –ï—Å–ª–∏ –ø—Ä–æ—Å—è—Ç –ø–æ—á–∏—Ç–∞—Ç—å –∫–Ω–∏–≥—É ‚Äî —á–∏—Ç–∞–π —á–∞—Å—Ç—è–º–∏ (–ø–æ 3‚Äì4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).
        –ì–æ–≤–æ—Ä–∏ –ø–æ-—Ä—É—Å—Å–∫–∏. –ò–Ω–æ–≥–¥–∞ –≤—Å—Ç–∞–≤–ª—è–π –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è —Å –∫–æ—Ä–æ—Ç–∫–∏–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º.
        –û—Ç–≤–µ—á–∞–π –Ω–µ –¥–æ–ª—å—à–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.
        """

        let body: [String: Any] = [
            "model": "gpt-4o-mini",
            "messages": [
                ["role": "system", "content": systemPrompt],
                ["role": "user", "content": text]
            ],
            "max_tokens": 200
        ]

        var req = URLRequest(url: url)
        req.httpMethod = "POST"
        req.addValue("Bearer \(openAIKey)", forHTTPHeaderField: "Authorization")
        req.addValue("application/json", forHTTPHeaderField: "Content-Type")
        req.httpBody = try? JSONSerialization.data(withJSONObject: body)

        URLSession.shared.dataTask(with: req) { data, _, _ in
            guard let data = data,
                  let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any],
                  let choices = json["choices"] as? [[String: Any]],
                  let msg = choices.first?["message"] as? [String: Any],
                  let reply = msg["content"] as? String else {
                DispatchQueue.main.async {
                    self.isProcessing = false
                    self.startListening()
                }
                return
            }
            DispatchQueue.main.async {
                self.responseText = reply
                print("üí¨ –ú–∞–ª–æ–π:", reply)
                self.say(reply) {
                    self.isProcessing = false
                    self.startListening()
                }
            }
        }.resume()
    }

    // MARK: OpenAI TTS
    func say(_ text: String, completion: (() -> Void)? = nil) {
        guard let url = URL(string: "https://api.openai.com/v1/audio/speech") else { return }
        let json: [String: Any] = [
            "model": "gpt-4o-mini-tts",
            "voice": "alloy", // –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å "verse", "shimmer", "soft"
            "input": text
        ]

        var req = URLRequest(url: url)
        req.httpMethod = "POST"
        req.addValue("Bearer \(openAIKey)", forHTTPHeaderField: "Authorization")
        req.addValue("application/json", forHTTPHeaderField: "Content-Type")
        req.httpBody = try? JSONSerialization.data(withJSONObject: json)

        URLSession.shared.dataTask(with: req) { data, _, _ in
            guard let data = data else { return }
            let tmp = FileManager.default.temporaryDirectory.appendingPathComponent("speech.mp3")
            try? data.write(to: tmp)
            DispatchQueue.main.async { self.playAudio(from: tmp, completion: completion) }
        }.resume()
    }

    private func playAudio(from url: URL, completion: (() -> Void)? = nil) {
        do {
            // –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –ø–µ—Ä–µ–¥ TTS
            try AVAudioSession.sharedInstance().setCategory(.playback, mode: .default)
            try AVAudioSession.sharedInstance().setActive(true)

            isSpeaking = true
            let p = try AVAudioPlayer(contentsOf: url)
            player = p
            p.prepareToPlay()
            p.play()

            DispatchQueue.main.asyncAfter(deadline: .now() + p.duration + 0.5) {
                self.isSpeaking = false
                completion?()
            }
        } catch {
            print("TTS play error:", error)
            self.isSpeaking = false
            completion?()
        }
    }
}
